{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Machine Learning Models\n",
    "Here we take two machine learning models, in this case active SageMaker endpoints, and compare the performance of these models.\n",
    "\n",
    "From a research perspective, we are interested in generating ROC curves. These will let us understand how the 2 models perform in terms of their false positives and true positives.\n",
    "\n",
    "From a practical perspective, we want to understand how the model is handling various types of prediction circumstances. We want to understand how the models handle different types of scenarios, and we will use its performance to increase the model over time.\n",
    "\n",
    "In the best case, both the research and practical perspectives work together to increase the model and its prediction generation in the real world, solving the problem for the best overall outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker as sm\n",
    "\n",
    "sess = sm.Session()\n",
    "\n",
    "linear_endpoint = sm.predictor.RealTimePredictor('linear-learner-2018-10-23-15-29-37-619', sess)\n",
    "balanced_endpoint = sm.predictor.RealTimePredictor(\"linear-learner-2018-10-23-16-47-29-308\", sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"../Data/fewer_labeled_rows_by_block.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_training_sets(data):\n",
    "    ys = np.array(data[\"Target\"]).astype(\"float32\")\n",
    "    \n",
    "    ys -= 1\n",
    "        \n",
    "    drop_list = [\"Target\", \"Date\", \"Primary Type\"]\n",
    "    \n",
    "    xs = np.array(data.drop(drop_list, axis=1)).astype(\"float32\")\n",
    "    \n",
    "    np.random.seed(0)\n",
    "\n",
    "    train_features, test_features, train_labels, test_labels = train_test_split(\n",
    "    xs, ys, test_size=0.2)\n",
    "    \n",
    "    val_features, test_features, val_labels, test_labels = train_test_split(\n",
    "    test_features, test_labels, test_size=0.5)\n",
    "    \n",
    "    return train_features, test_features, train_labels, test_labels, val_features, val_labels\n",
    "     \n",
    "    \n",
    "train_features, test_features, train_labels, test_labels, val_features, val_labels = create_training_sets(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def evaluate_metrics(predictor, test_features, test_labels):\n",
    "    \"\"\"\n",
    "    Evaluate a model on a test set using the given prediction endpoint. Display classification metrics.\n",
    "    \"\"\"\n",
    "    # split the test dataset into 100 batches and evaluate using prediction endpoint\n",
    "    prediction_batches = [predictor.predict(batch) for batch in np.array_split(test_features, 100)]\n",
    "\n",
    "    # parse protobuf responses to extract predicted labels\n",
    "    extract_label = lambda x: x.label['predicted_label'].float32_tensor.values\n",
    "    test_preds = np.concatenate([np.array([extract_label(x) for x in batch]) for batch in prediction_batches])\n",
    "    test_preds = test_preds.reshape((-1,))\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = (test_preds == test_labels).sum() / test_labels.shape[0]\n",
    "    \n",
    "    # calculate recall for each class\n",
    "    recall_per_class, classes = [], []\n",
    "    for target_label in np.unique(test_labels):\n",
    "        recall_numerator = np.logical_and(test_preds == target_label, test_labels == target_label).sum()\n",
    "        recall_denominator = (test_labels == target_label).sum()\n",
    "        recall_per_class.append(recall_numerator / recall_denominator)\n",
    "        classes.append(label_map[target_label])\n",
    "    recall = pd.DataFrame({'recall': recall_per_class, 'class_label': classes})\n",
    "    recall.sort_values('class_label', ascending=False, inplace=True)\n",
    "\n",
    "    # calculate confusion matrix\n",
    "    label_mapper = np.vectorize(lambda x: label_map[x])\n",
    "    confusion_matrix = pd.crosstab(label_mapper(test_labels), label_mapper(test_preds), \n",
    "                                   rownames=['Actuals'], colnames=['Predictions'], normalize='index')\n",
    "\n",
    "    # display results\n",
    "    sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix')  \n",
    "    ax = recall.plot(kind='barh', x='class_label', y='recall', color='steelblue', title='Recall', legend=False)\n",
    "    ax.set_ylabel('')\n",
    "    print('Accuracy: {:.3f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'linear_predictor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f4735fe75b19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_predictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'linear_predictor' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate_metrics(linear_, test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
